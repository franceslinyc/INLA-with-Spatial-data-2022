---
title: "INLA"
author: "Frances Lin"
date: "June 2022"
output: pdf_document
header-includes: \usepackage{setspace}\onehalfspacing
---

## Background and Introduction 

The steps involving the Bayesian inference may appear easy and straightforward: updating prior believes about the unknown parameters with observed data and obtaining the posterior distribution for the parameters. However, this is much harder to do in practice since solutions in closed form may not be determined. 

Then, the simulation-based inference through the idea of MCMC (Markov chain Monte Carlo) was introduced and represented a breakthrough in Bayesian inference (Robert & Casella, 1999) in the early 1990s. MCMC tools such as `WinBugs` (Spiegelhalter et al., 1995), `JAGS` (Plummer, 2016), and `stan` (Stan Development Team, 2015) have also been developed. Bayesian statistics has become popular. 

However, 

As a result, 

INLA is a fast alternative to MCMC for a class of LGMs (latent Gaussian Models). 

INLA is both faster and more accurate than MCMC alternatives. 




## Notation 

### Bayesian Inference 

The posterior distribution is proportional to the likelihood function multiples by the prior distribution **(Need citation)** 

$$
f(\theta|y) = \frac{p(y|\theta) p(\theta)} {\int p(y|\theta) p(\theta) d\theta} \propto p(y|\theta) p(\theta), 
$$
where $p(y|\theta)$ is the likelihood function, $p(\theta)$ is the prior, and ${\int p(y|\theta) p(\theta) d\theta}$ is the normalizing constant. 

Based on the posterior distribution, relevant statistics for the parameters of interest such as marginal distribution, means, variances, quantiles, credibility intervals, etc. can be obtained (Rue et al., 2017).

However, the integral is generally intractable, thus requiring the use of numerical methods such as MCMC. 

### Latent Gaussian Models

The latent Gaussian Models (LGMs) is a class of three-state Bayesian hierarchical models. 

Observations $y$ is assumed to be conditionally independent, given a latent Gaussian random field $x$ (joint distribution of all parameters) and hyperparameters $\theta_1$: 

$$
y | x, \theta_1 \sim \prod_{i \in I} p (y_i | x_i, \theta_1). \ \ \ \ \ _{likelihood}
$$

The versatility of the model class can be specified through the latent Gaussian field: 
$$
x | \theta_2 \sim N(\mu(\theta_2), Q^{-1}(\theta_2)), \ \ \ \ \ _{latent \ field}
$$
where the precision matrix $Q^{-1} = \Sigma$, which captures the underlying dependence structure of the data. 

The hyperparameters of the non-Gaussian latent field: 

$$
\theta = {(\theta_1, \theta_2)} \sim p(\theta) \ \ \ \ \ _{hyperpriors}
$$

controls the latent Gaussian field and/or the likelihood of the data. 

The posterior becomes: 

$$
p(x, \theta | y) \propto p(\theta) p(x | \theta) \prod_{i \in I} p(y_i | x_i, \theta).
$$

The following assumptions are needed for computational reasons and to ensure accurate approximations: 

1. The number of hyperparameters $\theta$ is small (~ $2$ to $5$ but $< 20$).

2. The distribution of the latent field is Gaussian and is close to a Gaussian Markov random field (GMRF) when $n$ is high ($10^3$ to $10^5$).  

3. Each observation $y_i$ depends only on one component of the latent field $x_i$, and most components of $x$ will not be observed. 



### Additive Models

LGMS are an umbrella class that generalizes the large number of related additive and/or generalized linear models. Consider the generalized linear model setup, for example,

$$
y \sim \prod_i^N p(y_i | x_i, \theta)
$$

$$
g(\mu_i) = \eta_i = \alpha + \sum_j \beta_j z_{ij} + \sum_k f_{k, jk(i)},
$$
where $\alpha$ is the overall intercept, $z$ are fixed predictors with linear effects $\{\beta_j\}$ and $\{f_k\}$ represents specific Gaussian processes such as random effects, spatially or temporally correlated effects, smoothing splines, measure errors, etc.





### Gaussian Markov Random Fields

GMRFs

### Additive Models and Gaussian Markov Random Fields

### Laplace Approximations 

## INLA 




## Applications 

The `R` package `R-INLA` have found spatial applications in a wide variety of fields such as environment, ecology, disease mapping, medical imaging, public health, cancer research, energy, economics, risk analysis, etc. 

Some selected examples include: environmental risk factors to liver Fluke in cattle (Innocent et al., 2017); modelling recovering fish populations (Boudreau et al., 2017); polio-virus eradication in Pakistan (Mercer et al., 2017); cortical surface fMRI data (Mejia et al., 2017); socio demographic and geographic impact of HPV vaccination (Rutten et al., 2017); topsoil metals and cancer mortality (Lopez-Abente et al., 2017) with spatially misaligned data; ethanol and gasoline pricing (Laurini, 2017); applications in spatial econometrics (Bivand et al., 2014; Gomez-Rubio et al., 2015; Gomez-Rubio et al., 2014); probabilistic prediction of wind power (Lenzi et al., 2017); modeling landslides as point processes (Lombardo et al., 2018); predicting extreme rainfall events in space and time (Opitz et al., 2018), etc.




\newpage

## Reference

[Bakka, H., Rue, H., Fuglstad, G. A., Riebler, A., Bolin, D., Illian, J., Krainski, E., & Lindgren, F. (2018). Spatial modeling with R‐INLA: A review. Wiley Interdisciplinary Reviews: Computational Statistics, 10(6), e1443.](https://arxiv.org/pdf/1802.06350.pdf)

Morrison, K. (2017). A gentle INLA tutorial. Precision Analytics. [https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/.](https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/)

[Rue, H., Riebler, A., Sørbye, S. H., Illian, J. B., Simpson, D. P., & Lindgren, F. K. (2017). Bayesian computing with INLA: a review. Annual Review of Statistics and Its Application, 4, 395-421.](https://www.annualreviews.org/doi/full/10.1146/annurev-statistics-060116-054045?casa_token=sIho3hU0GtwAAAAA:hAwMg1zvZyqaxk00jCdYyQFfzc5pDwOz5hKK1AzVjjnIY6CANQ8YWbaCC0EQk-5UteoMDy-aHnUI)


